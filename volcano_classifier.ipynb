{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cbe7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This program classifies the type of volcano based on the vibrations detected by sensors.\n",
    "\n",
    "# Getting libraries\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7945b28f",
   "metadata": {
    "code_folding": [
     54,
     107
    ]
   },
   "outputs": [],
   "source": [
    "class VolcanoClassifier:\n",
    "    \"\"\"\n",
    "    This class is used to classify the type of volcano based on the vibrations detected by sensors. Contains methods to\n",
    "    preprocess the data, train the model, and test the model, as well as methods to save the predictions and the model.\n",
    "    The model used is a Random Forest Classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inizialize the class and define the model to use\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "        self.model_path = \"models/\"\n",
    "        self.data_train = pd.read_csv('data/jm_train.csv')\n",
    "        self.data_test = pd.read_csv('data/jm_X_test.csv')\n",
    "        self.data_local_train = pd.DataFrame()\n",
    "        self.data_local_test = pd.DataFrame()\n",
    "        self.best_params = dict()\n",
    "        self.rdn_number = np.random.randint(200)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        This function preprocesses the data to be used in the model. Perform a normal scaling to both, \n",
    "        train and test data set. \n",
    "        The training set is split into two different sets, based on the class distribution:\n",
    "            1.- 80% of the data is just for local training.\n",
    "            2.- 20% of the data is only for local testing.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # Scale the training data\n",
    "        scale = StandardScaler()\n",
    "        scale.fit(self.data_train.drop(\"target\", axis=1))\n",
    "        x_scaled = scale.transform(self.data_train.drop(\"target\", axis=1))\n",
    "        columns_names = self.data_train.columns\n",
    "        self.data_train = pd.concat([pd.DataFrame(x_scaled), self.data_train.target], axis=1)\n",
    "        self.data_train.columns = columns_names\n",
    "            \n",
    "        # Scale the test data\n",
    "        scale.fit(self.data_test)\n",
    "        x_test_scaled = scale.transform(self.data_test)\n",
    "        columns_names = self.data_test.columns\n",
    "        self.data_test = pd.DataFrame(x_test_scaled)\n",
    "        self.data_test.columns = columns_names\n",
    "\n",
    "#       # Ensuring a sample well distributed  \n",
    "        def sample(group):\n",
    "            return group.sample(frac=0.8, random_state= self.rdn_number)\n",
    "\n",
    "        # Define a new training and test set\n",
    "        self.data_local_train = self.data_train.groupby(\"target\", as_index=False).apply(sample).reset_index(drop=True)\n",
    "        self.data_local_test = self.data_train.drop(self.data_local_train.index)\n",
    "        \n",
    "\n",
    "    def grid_search(self):\n",
    "        \"\"\"\n",
    "        This function is used to optimize the hyper-parameters of the Random Forest model using GridSearchCV.\n",
    "        It performs only over the local training set.\n",
    "        \"\"\"\n",
    "\n",
    "        def params_grid():\n",
    "            \"\"\"\n",
    "            This function defines the hyper-parameters to be used in the GridSearchCV.\n",
    "            \"\"\"\n",
    "            # Number of trees in random forest\n",
    "#             n_estimators = [int(x) for x in np.linspace(start=10, stop=1500, num=10)]\n",
    "            n_estimators = [int(x) for x in np.linspace(start=672, stop=100, num=1)]\n",
    "            # Number of features to consider at every split\n",
    "#             max_features = ['auto', 'sqrt']\n",
    "            max_features = ['sqrt']\n",
    "            # Maximum number of levels in tree\n",
    "#             max_depth = [int(x) for x in np.linspace(1, 63, num=9)]\n",
    "            max_depth = [int(x) for x in np.linspace(8, 50, num=1)]\n",
    "#             max_depth.append(None)\n",
    "            # Minimum number of samples required to split a node\n",
    "#             min_samples_split = [2, 3, 5]\n",
    "            min_samples_split = [2]\n",
    "            # Minimum number of samples required at each leaf node\n",
    "#             min_samples_leaf = [1, 2, 4]\n",
    "            min_samples_leaf = [1]\n",
    "            # Method of selecting samples for training each tree\n",
    "#             bootstrap = [True, False]\n",
    "            bootstrap = [True]\n",
    "#             criterion = ['gini', 'entropy']\n",
    "            criterion = ['entropy']\n",
    "\n",
    "            # Create the random grid\n",
    "            random_grid_ = {'n_estimators': n_estimators,\n",
    "                            'max_features': max_features,\n",
    "                            'max_depth': max_depth,\n",
    "                            'min_samples_split': min_samples_split,\n",
    "                            'min_samples_leaf': min_samples_leaf,\n",
    "                            'bootstrap': bootstrap,\n",
    "                            'criterion': criterion}\n",
    "            \n",
    "            return random_grid_\n",
    "\n",
    "        random_grid = params_grid()\n",
    "        rf = RandomForestClassifier()\n",
    "        grid_search = GridSearchCV(estimator=rf, param_grid=random_grid, cv=2, n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(self.data_local_train, self.data_local_train.target)\n",
    "        \n",
    "        self.best_params = grid_search.best_params_\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        This function trains the model using the local training set and calculates the F1 score over the local test set.\n",
    "        A stratified Kfold is used to train the model with the hyper-parameter optimized over several training sets. \n",
    "        The model with the best F1 score is selected to use in the evaluation step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the data into several stratified training and test set\n",
    "        sk_fold = StratifiedKFold(n_splits=9, shuffle=True, random_state=self.rdn_number)\n",
    "\n",
    "        # Create the model\n",
    "        temp_model = RandomForestClassifier(\n",
    "            n_estimators=self.best_params[\"n_estimators\"],\n",
    "            max_depth=self.best_params[\"max_depth\"],\n",
    "            max_features=self.best_params[\"max_features\"],\n",
    "            criterion=self.best_params[\"criterion\"],\n",
    "            min_samples_split=self.best_params[\"min_samples_split\"],\n",
    "            min_samples_leaf=self.best_params[\"min_samples_leaf\"],\n",
    "            bootstrap=self.best_params[\"bootstrap\"]\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        x_train = self.data_local_train.drop(\"target\", axis=1)\n",
    "        y_train = self.data_local_train.target\n",
    "\n",
    "        prediction = None\n",
    "        temp_f1_score = 0\n",
    "        for (train_index, test_index) in sk_fold.split(self.data_local_train, self.data_local_train['target']):\n",
    "            \n",
    "            temp_model.fit(x_train.iloc[train_index], y_train.iloc[train_index])\n",
    "            prediction = temp_model.predict(x_train.iloc[test_index])\n",
    "            f1_score = metrics.f1_score(y_train.iloc[test_index], prediction, average='macro')\n",
    "            \n",
    "            if f1_score > temp_f1_score:\n",
    "                temp_f1_score = f1_score\n",
    "                self.model = temp_model\n",
    "\n",
    "        # Save the model\n",
    "        with open(self.model_path + \"rfc_model\", 'wb') as file:\n",
    "            pickle.dump(self.model, file)\n",
    "            \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        This function evaluates the model over the local test set (to calculate the final f1_score) as well as the \n",
    "        test set from the competition, which targets is unknown. Finally, save the results for competition in the file\n",
    "        \"y_pred\" in the data folder and the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        x_test = self.data_local_test.drop(\"target\", axis=1)\n",
    "        y_test = self.data_local_test.target\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred = self.model.predict(x_test)\n",
    "        f1_score = metrics.f1_score(y_test, y_pred, average='macro')\n",
    "        prinnit(\"Local TEST\", f1_score)\n",
    "\n",
    "        # Save the results\n",
    "        y_pred = self.model.predict(self.data_test)\n",
    "        pd.DataFrame(y_pred, columns=[\"target\"]).to_csv('data/y_pred.csv', header=True, index=False)\n",
    "        \n",
    "        # Save the model\n",
    "        with open(self.model_path + \"rfc_model\", 'wb') as file:\n",
    "            pickle.dump(self.model, file)\n",
    "        \n",
    "        return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9b3b5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local TEST 0.8726885844346575\n",
      "Wall time: 31.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8726885844346575"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "vc = VolcanoClassifier()\n",
    "vc.preprocess_data()\n",
    "vc.grid_search()\n",
    "vc.train_model()\n",
    "vc.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46d3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}